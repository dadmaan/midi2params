{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Evaluation of midi2params\n",
    "Notebook to qualitatively evaluate our trained midi2params model. This has a lot of extra details and is not *that* user-friendly, so be warned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line basically \"injects\" the global state of this script\n",
    "# at the end into this notebook\n",
    "%run ../midi2params/interact.py\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first get our batch and see what's in it. Here you can choose which example we want to look at (with `i`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 7\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, play the original audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.util import preview_audio\n",
    "\n",
    "audio = to_numpy(batch['audio'][i])[..., np.newaxis]\n",
    "preview_audio(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 5, audio.flatten().shape[0]), audio.flatten())\n",
    "plt.yticks([])\n",
    "plt.title('Audio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesized from features extracted with DDSP\n",
    "Now, synthesize with DDSP from the features extracted with DDSP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract synthesis parameters\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wavegenie.util import extract_ddsp_synthesis_parameters\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "audio_parameters = extract_ddsp_synthesis_parameters(np.zeros((1, 1000)))\n",
    "\n",
    "plt.title('f0(t)')\n",
    "plt.plot(audio_parameters['f0_hz'], color='orange')\n",
    "plt.xlim(0, 1250)\n",
    "plt.show()\n",
    "plt.title('l(t)')\n",
    "plt.plot(audio_parameters['loudness_db'])\n",
    "plt.xlim(0, 1250)\n",
    "plt.ylim(-120, 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract synthesis parameters\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.util import extract_ddsp_synthesis_parameters\n",
    "\n",
    "audio_parameters = extract_ddsp_synthesis_parameters(np.zeros((1,1000)))\n",
    "\n",
    "plt.title('f0(t)')\n",
    "plt.plot(audio_parameters['f0_hz'], color='orange')\n",
    "plt.xlim(0, 1250)\n",
    "plt.show()\n",
    "plt.title('l(t)')\n",
    "plt.plot(audio_parameters['loudness_db'])\n",
    "plt.xlim(0, 1250)\n",
    "plt.ylim(-120, 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = lambda x: (x - x.min()) / (x.max() - x.min())\n",
    "plt.figure(figsize=(16, 4))\n",
    "N = len(batch['pitches'][i][100:])\n",
    "plt.scatter(np.linspace(0, 5, N), normalize(batch['pitches'][i][100:]), s=2)\n",
    "#plt.plot(normalize(audio_parameters['f0_hz']))\n",
    "plt.xlim(0, 5)\n",
    "plt.yticks([])\n",
    "plt.title('MIDI Piano Roll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "from wavegenie.util import load_ddsp_model\n",
    "\n",
    "#model = load_ddsp_model('Violin')\n",
    "ckpt_path = '/juice/scr/rjcaste/curis/ddsp/ddsp/colab/checkpoints/Experiment13/2020-09-19 08:43:18/stopped'\n",
    "model = load_ddsp_model(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resynthesize parameters\n",
    "\n",
    "from wavegenie.util import synthesize_ddsp_audio\n",
    "\n",
    "resynth = synthesize_ddsp_audio(model, audio_parameters)\n",
    "\n",
    "preview_audio(resynth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesize with *heuristically generated* features from MIDI\n",
    "Now, synthesize with DDSP from the features *heuristically generated* from associated MIDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_loud(beats, length=1250, decay=True):\n",
    "    arrs = []\n",
    "    length = 2500\n",
    "    base = -30\n",
    "    decay_rate = -0.01 # decays -1 per timestep/index\n",
    "    #notelength = 0.7\n",
    "    ld_arr = np.full((length), -120)\n",
    "    for i, beat in enumerate(beats):\n",
    "        if i == len(beats) - 1:\n",
    "            next_beat = length\n",
    "        else:\n",
    "            next_beat = beats[i + 1]\n",
    "        ld_arr[beat:next_beat] = np.linspace(base, base + decay_rate * (next_beat - beat), next_beat - beat)\n",
    "\n",
    "    return ld_arr\n",
    "\n",
    "\n",
    "def gen_heuristic(batch, i=0):\n",
    "    \"\"\"\n",
    "    Take a batch containing 'pitches', 'onset_arr', and 'offset_arr' and\n",
    "    turn them into f0 and loudness heuristically.\n",
    "    \"\"\"\n",
    "    #onsets = np.concatenate(([10], np.where(batch['onset_arr'][i] == 1)[0]))\n",
    "    onsets = np.where(batch['onset_arr'][i] == 1)[0]\n",
    "    if len([i for i in onsets if i < 30]) == 0:\n",
    "        onsets = np.concatenate(([10], onsets))\n",
    "\n",
    "    #np.save(os.path.join(SAVE_PATH, 'onsets-{}.npy'.format(i)), onsets)\n",
    "    ld = generate_loud(onsets)\n",
    "    pitches = copy.deepcopy(batch['pitches'][i])\n",
    "    f0 = p2f(pitches)\n",
    "    return f0, ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_h, ld_h = gen_heuristic(batch, i=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('f0(t)')\n",
    "f0_h_sin = np.abs(np.array(f0_h) + 3 * np.sin(np.arange(2500) * .15))\n",
    "plt.plot(f0_h, color='orange')\n",
    "plt.plot(f0_h_sin, color='red')\n",
    "#plt.xlim(1, 2)\n",
    "plt.xlim(0, 1250)\n",
    "plt.show()\n",
    "plt.title('l(t)')\n",
    "plt.plot(audio_parameters['loudness_db'], label='loudness (ground truth)')\n",
    "plt.plot(ld_h, label='loudness (generated)')\n",
    "#plt.xlim(1, 2)\n",
    "plt.ylim(-120, 0)\n",
    "plt.xlim(0, 1250)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_f0_h_sin = torch.FloatTensor(f0_h_sin)\n",
    "#torch_f0_h_sin.dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristic_parameters = {\n",
    "    'f0_hz': torch_f0_h_sin.type(torch.float32),\n",
    "    'loudness_db': ld_h.astype(np.float32)\n",
    "}\n",
    "params = heuristic_parameters\n",
    "params = {\n",
    "    'f0_hz': batch['f0'][i],\n",
    "    'loudness_db': batch['loudness_db'][i]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resynthesize parameters\n",
    "\n",
    "from wavegenie.util import synthesize_ddsp_audio\n",
    "\n",
    "heuristic_resynth = synthesize_ddsp_audio(model, params)\n",
    "\n",
    "preview_audio(heuristic_resynth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesize from feature outputs from *learned model*\n",
    "Now, synthesize with DDSP from the features generated from the associated MIDI *with our trained model*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/juice/scr/rjcaste/curis/wavegenie/wandb/run-20200925_110018-1yf32a9v/best_model.pt'\n",
    "# this run corresponds to `atomic-salad-1884` on wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model and generate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_best_model(config, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.device == 'cuda':\n",
    "    for k, arr in batch.items():\n",
    "        batch[k] = torch.Tensor(arr.float()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_pred, ld_pred = midi2params(best_model, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, arr in batch.items():\n",
    "    batch[k] = to_numpy(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i)\n",
    "f0 = batch['f0'][i]\n",
    "ld = batch['loudness_db'][i]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('f0(t) comparison')\n",
    "plt.plot(f0_pred[i], label='f0 (generated)')\n",
    "plt.plot(f0, alpha=0.5, label='f0 (ground truth)')\n",
    "plt.xlim(0, 1250)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('l(t)')\n",
    "plt.plot(ld_pred[i], label='loudness (generated)')\n",
    "plt.plot(ld, alpha=0.5, label='loudness (ground truth)')\n",
    "plt.xlim(0, 1250)\n",
    "plt.ylim(-120, 0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'f0_hz': f0_pred[i],\n",
    "    'loudness_db': ld_pred[i]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resynthesize parameters\n",
    "\n",
    "from wavegenie.util import synthesize_ddsp_audio, preview_audio\n",
    "\n",
    "new_model_resynth = synthesize_ddsp_audio(model, train_params)\n",
    "\n",
    "preview_audio(new_model_resynth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: This following code section for quantization is under development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(signal, max_=0, min_=-120, bins=120):\n",
    "    signal = copy.deepcopy(signal)\n",
    "    signal = to_numpy(signal)\n",
    "    normalized = (signal - min_) / (max_ - min_)\n",
    "    \n",
    "    quantized_onehot = np.stack([np.logical_and(i / bins < normalized, normalized < ((i + 1) / bins)) for i in range(bins)])\n",
    "    quantized_int = quantized_onehot.argmax(0)\n",
    "    \n",
    "    # affine transform back to original signal\n",
    "    quantized_signal = (max_ - min_) * (quantized_int / bins) + min_\n",
    "    \n",
    "    return quantized_signal.astype(np.float32)\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]\n",
    "\n",
    "\n",
    "def quantize(signal, max_=0, min_=-120, bins=120):\n",
    "    signal = copy.deepcopy(signal)\n",
    "    values = np.linspace(min_, max_, bins)\n",
    "    \n",
    "    return np.array([find_nearest(values, x) for x in np.array(signal)]).astype(np.float32)\n",
    "\n",
    "def quantize(signal, max_=0, min_=-120, bins=120):\n",
    "    signal = copy.deepcopy(signal)\n",
    "    signal = to_numpy(signal)\n",
    "    normalized = (signal - min_) / (max_ - min_)\n",
    "    \n",
    "    quantized = np.round(normalized * bins) / bins * (max_- min_) + min_\n",
    "    \n",
    "    return quantized\n",
    "\n",
    "quantized_params = {\n",
    "    'f0_hz': batch['f0'][i],\n",
    "    'loudness_db': quantize(batch['loudness_db'][i], bins=100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resynthesize parameters\n",
    "\n",
    "from wavegenie.util import synthesize_ddsp_audio\n",
    "\n",
    "quantized_resynth = synthesize_ddsp_audio(model, quantized_params)\n",
    "\n",
    "preview_audio(quantized_resynth)\n",
    "preview_audio(resynth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(batch['loudness_db'][i], label='batch')\n",
    "plt.plot(audio_parameters['loudness_db'], label='extracted')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, all of them side-by-side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_audio(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct DDSP Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_audio(resynth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct DDSP Features after dequant(quant(x, bins=num_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_audio(quantized_resynth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristically Generated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_audio(heuristic_resynth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features from Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_audio(new_model_resynth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking progress on loudness\n",
    "Here, we check:\n",
    "- heuristic f0 + real loudness\n",
    "- heuristic f0 + generated loudness\n",
    "- heuristic f0 + real loudness sent through quantization schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference (Real F0, Real Loudness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp0_params = {\n",
    "    'f0_hz': batch['f0'][i],\n",
    "    'loudness_db': batch['loudness_db'][i]\n",
    "}\n",
    "\n",
    "exp0_resynth = synthesize_ddsp_audio(model, exp0_params)\n",
    "\n",
    "preview_audio(exp0_resynth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Heuristic F0, Real Loudness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_params = {\n",
    "    'f0_hz': f0_h,\n",
    "    'loudness_db': batch['loudness_db'][i]\n",
    "}\n",
    "\n",
    "exp1_resynth = synthesize_ddsp_audio(model, exp1_params)\n",
    "\n",
    "preview_audio(exp1_resynth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Heuristic F0, Generated Loudness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_params = {\n",
    "    'f0_hz': batch['f0'][i],\n",
    "    'loudness_db': ld_pred[i]\n",
    "}\n",
    "\n",
    "exp2_resynth = synthesize_ddsp_audio(model, exp2_params)\n",
    "\n",
    "preview_audio(exp2_resynth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Heuristic F0, dequant(quant(Real Loudness)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3_params = {\n",
    "    'f0_hz': f0_h,\n",
    "    'loudness_db': quantize(batch['loudness_db'][i], bins=50)\n",
    "}\n",
    "\n",
    "exp3_resynth = synthesize_ddsp_audio(model, exp3_params)\n",
    "\n",
    "preview_audio(exp3_resynth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization procedure doesn't actually distort the sound much. This leads me to believe that the model still has some areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavegenie37",
   "language": "python",
   "name": "wavegenie37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
